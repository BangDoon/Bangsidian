Progressive Growing of GANs
[[GAN]]

styleGAN의 기초가 되는 모델

## Abstract

기존의 GAN 에는 이미지가 고해상도 일수록 생성자가 판별자를 속이기 어려워 지고, 학습 속도도 매우 느려졌으며, 학습이 불안정한 문제점이 있었습니다. 이를 해결하기 위한 방법으로 본 논문에서는 이름에서 볼 수 있듯이 생성자(generator) 와 판별자(discriminator)를 점진적으로 학습 시키는 방법을 제안합니다.


## Progressive Growing of GANs

![[pggan1.gif]]

위 그림처럼 PGGAN은 **저해상도의 이미지에서 시작하여 점점 고해상도의 이미지를 학습**하게 됩니다. 이를 다르게 말하면 **처음에는 large scale(큼직큼직 하게)의 정보들을 학습하고, 점차 fine scale(세세하게)의 정보들을 학습하게 되는 것**입니다. 또 생성자와 판별자의 네트워크는 대칭형태를 이루고 있으며, 모든 layer는 학습시에 동결시키지 않고 학습을 진행하게 됩니다.


![[Pasted image 20240110001526.png]]

위 그림은 점진적 학습을 위해서 layer를 smooth fade in 하는 것을 보여주고 있습니다. smooth fade in이 필요한 이유는 예를 들어 16x16 layer가 잘 학습이 되어 32x32 layer를 추가하고자 할 때, 무작정 그냥 추가하게 된다면 전혀 학습이 안된 32x32 layer의 간섭으로 잘 학습된 16x16 layer에 영향을 줄 수 있기 때문입니다. 이를 해결하기 위해 이전에 학습시킨 레이어의 출력 이미지를 사용하게 됩니다.

(b) : layer를 추가할 때 잘 학습된 16x16 layer의 출력 이미지를 32x32로 늘린다음 32x32 layer의 출력 이미지와 합쳐줍니다. 이 때 각 출력 이미지의 픽셀 값에 α를 곱하여 합쳐주는데, α 값은 0에서 1로 점차적으로 증가하며 이는 영향력을 나타냅니다. 처음에는 32x32 layer의 출력 이미지가 학습이 안되어 있으니 영향력이 0이고, 잘 학습된 16x16 layer의 출력 이미지는 영향력이 1이 되는 것입니다. 이는 학습을 하면 할수록 반전됩니다.

(c) : 학습이 일정수준 이상 되었으면 이전 layer의 보조를 빼고, 그대로 학습을 진행하며 파인 튜닝을 진행하게 됩니다.

위에서 본 점진적 학습을 통해서 모델은 보다 **안정적으로 학습**이 가능해졌고, 한번에 고해상도 이미지가 들어와서 특징을 찾는 것 보다, 저해상도 부터 차츰차츰 특징을 찾아가는게 더 쉬움으로 간단해지고, 학습 속도가 빨라졌습니다.