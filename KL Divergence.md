Kullback_Leibler Divergence

KL(Kullback-Leibler) divergence는 두 확률 분포 사이의 차이를 측정하는 데 사용되는 통계적인 개념입니다. 이는 정보 이론과 확률 분포 분석에서 중요한 개념 중 하나입니다. KL divergence는 두 확률 분포가 얼마나 비슷한지 또는 다른지를 측정합니다.

두 확률 분포 P(x)와 Q(x)가 주어졌을 때, KL divergence는 다음과 같이 정의됩니다:
$$KL(P(x)|Q(x)) = \int \Sigma_x{P(x)}{Q(x)}P(x)dx$$

$$KL(P(x)|Q(x)) = \int \textrm P(x){log}\frac{P(x)}{Q(x)}dx$$